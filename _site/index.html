<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Qiao's Home Page - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Qiao's Home Page">
<meta property="og:title" content="Qiao's Home Page">


  <link rel="canonical" href="https://github.com/pages/qianqianai/qianqianai.github.io/">
  <meta property="og:url" content="https://github.com/pages/qianqianai/qianqianai.github.io/">











<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/me_cat.png" class="author__avatar" alt="Qian Qiao">
  </div>

  <div class="author__content">
    <h3 class="author__name">Qian Qiao</h3>
    <p class="author__bio">Ph.D., Fudan University</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">My girlfriend and I have a cute cat ğŸ± named Yuanbao (å…ƒå®).</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Shanghai, China</li>
      
      
      
      
        <li><a href="mailto:joeqian@aliyun.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/qianqianai"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="xxxx"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:joeqian@aliyun.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/qianqianai"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="xxxx"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="About Me">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <h2 id="about-me">About Me</h2>

<p>My name is Qian Qiao (ä¹”è°¦). I am currently a researcher at Soul AILab. I received my Masterâ€™s degree from Soochow University in July 2025, supervised by Professors <a href="https://scst.suda.edu.cn/0e/e0/c11250a528096/page.htm">Fanzhang Li</a>. My previous research focused on multimodal understanding, image generation, text spotting, and few-shot learning. Additionally, I have four years of experience in blockchain technology and investment. <strong>Currently, my primary research interests are real-time video generation and dLLMs, and I am actively seeking collaborators in these fields!</strong></p>

<!-- <i style="color:#e74d3c">I will join [Weizhong Zhang](https://weizhonz.github.io/)'s team as a research assistant next year.</i>   -->

<!-- I am actively looking for a Ph.D. position or **research assistant (remote)** in 2025 Fall. Feel free to contact me if you are interested! -->
<ul>
  <li>ğŸ“§ Email: <a href="mailto:joeqian@aliyun.com">joeqian@aliyun.com</a>, <a href="mailto:qqiao@stu.suda.edu.cn">qqiao@stu.suda.edu.cn</a> (Since I have graduated, my academic email address will be deactivated.)</li>
  <li>ğŸ’¬ Wechat: Joeqqqqqqq</li>
  <li>ğŸ“± Phone: +86 18888178182</li>
</ul>

<h2 id="research-and-publications">Research and Publications</h2>
<p>To date, I have published over <strong>10</strong> scholarly papers as a first author or co-author, and I have actively participated in the review process of top-tier conferences and journals such as AAAI, ACL, EMNLP, ACM MM, ICASSP and PR, among others. My work primarily focuses on areas such as multimodal understanding, image generation, text spotting and few-shot learning.</p>

<!-- ## ğŸ“– Educations-->
<!-- + Master, School of Computer Science and Technology, **Soochow University**, Suzhou, China.-->
<!-- + Bachelor,School of Computer Science and Technology, **Soochow University**, Suzhou, China.-->
<h2 id="-honors-and-awards">ğŸ– Honors and Awards</h2>
<!-- - *2022.09* National Scholarships for Postgraduate Students. -->
<ul>
  <li>National Scholarship, 2024 (MS. student)</li>
  <li>Special Grade Scholarship, 2023 (MS. student)</li>
</ul>

<h2 id="academic-competitions">Academic Competitions</h2>
<ul>
  <li><strong>2024.06</strong>: My collaborator Yu Xie and I won <strong>three first places</strong> and <strong>one second place</strong> in the <strong>ICADR2024-Text Map</strong> Challenge (ICADR is one of the most authoritative conferences in the field of OCR), and we have been <strong>invited</strong> to present a technical report at ICADR2024.</li>
</ul>

<h2 id="-news">ğŸ”¥ News</h2>
<ul>
  <li><em>2025.01</em>: ğŸ‰ <code class="language-plaintext highlighter-rouge">QPruner:ProbabilisticDecision Quantization for StructuredPruning in Large Language Models</code> is accepted by NAACL 2025.</li>
  <li><em>2024.12</em>: ğŸ‰ <code class="language-plaintext highlighter-rouge">AIM: Let Any Multimodal Large Language Models Embrace Efficient In-Context Learning</code> is accepted by AAAI 2024.</li>
  <li><em>2024.11</em>: ğŸ‰ Invited by CogSci, ICME and IJCNN as Reviewer.</li>
  <li><em>2024.10</em>: ğŸ‰ ğŸ”¥ğŸ”¥ğŸ”¥ I honoured the national scholarship.</li>
  <li><em>2024.07</em>: ğŸ‰ Invited by AAAI as Reviewer.</li>
  <li><em>2024.08</em>: ğŸ‰ One paper is accepted by PRICAI 2024.</li>
  <li><em>2024.07</em>: ğŸ‰ ğŸ”¥ğŸ”¥ğŸ”¥ <code class="language-plaintext highlighter-rouge">DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training</code> is accepted by ACM MM 2024.</li>
  <li><em>2024.07</em>: ğŸ‰ Invited by EMNLP as Reviewer.</li>
  <li><em>2024.06</em>: ğŸ‰ Two papers are accepted by ICANN 2024.</li>
  <li><em>2024.06</em>: ğŸ‰ Two papers are submitted to EMNLP 2024 as co-author.</li>
  <li><em>2024.05</em>: ğŸ‰ Invited by NeurIPS as Reviewer.</li>
  <li><em>2024.04</em>: ğŸ‰ Attending ICASSP2024 in Seoul, South Korea.</li>
  <li><em>2024.04</em>: ğŸ‰ Two papers are accepted by ICME 2024.</li>
  <li><em>2024.03</em>: ğŸ‰ Invited by MM as Program Reviewer.</li>
</ul>

<h1 id="-publications">ğŸ“ Publications</h1>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2025</div><video src="https://github.com/user-attachments/assets/e55952e6-e1b2-44a5-9887-a89307a378da" width="320" controls="" loop=""></video></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation,</p>

        <p><strong>Zhe Kong</strong> *, Feng Gao *, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, Wenhan Luo</p>

        <p><em>Conference on Neural Information Processing Systems (NeurIPS), 2025.</em></p>

        <p><a href="https://arxiv.org/abs/2505.22647"><strong>[arxiv]</strong></a> <a href="https://github.com/MeiGen-AI/MultiTalk"><strong>[code]</strong></a> <a href="https://meigen-ai.github.io/multi-talk/"><strong>[project]</strong></a>  <a href="https://github.com/MeiGen-AI/MultiTalk"><img src="https://img.shields.io/github/stars/MeiGen-AI/MultiTalk?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Technical Report</div><img src="images/paper/infinitetalk.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing,</p>

        <p>Shaoshu Yang *, <strong>Zhe Kong</strong> *, Feng Gao *, Meng Cheng *, Xiangyu Liu *, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, Xiaoming Wei</p>

        <p><em>Technical Report, 2025.</em></p>

        <p><a href="https://arxiv.org/abs/2508.14033"><strong>[arxiv]</strong></a> <a href="https://github.com/MeiGen-AI/InfiniteTalk"><strong>[code]</strong></a> <a href="https://meigen-ai.github.io/InfiniteTalk/"><strong>[project]</strong></a>  <a href="https://github.com/MeiGen-AI/InfiniteTalk"><img src="https://img.shields.io/github/stars/MeiGen-AI/InfiniteTalk?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ACM SIGGRAPH 2025</div><img src="images/paper/dam-vsr.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution,</p>

        <p><strong>Zhe Kong</strong>, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo</p>

        <p><em>ACM SIGGRAPH, 2025.</em></p>

        <p><a href="https://arxiv.org/abs/2507.01012"><strong>[arxiv]</strong></a>  <a href="https://github.com/kongzhecn/DAM-VSR"><strong>[code]</strong></a>  <a href="https://kongzhecn.github.io/projects/dam-vsr/"><strong>[project]</strong></a>  <a href="https://github.com/kongzhecn/DAM-VSR"><img src="https://img.shields.io/github/stars/kongzhecn/DAM-VSR?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2024</div><img src="images/paper/omg.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models,</p>

        <p><strong>Zhe Kong</strong>, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, Wenhan Luo,</p>

        <p><em>European Conference on Computer Vision (ECCV), 2024.</em></p>

        <p><a href="https://arxiv.org/abs/2403.10983"><strong>[arxiv]</strong></a> <a href="https://github.com/kongzhecn/OMG"><strong>[code]</strong></a> <a href="https://kongzhecn.github.io/omg-project/"><strong>[project]</strong></a> <a href="https://huggingface.co/spaces/Fucius/OMG"><strong>[huggingFace demo]</strong></a>  <a href="https://github.com/kongzhecn/OMG"><img src="https://img.shields.io/github/stars/kongzhecn/OMG?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TCSVT</div><img src="images/paper/DTDA.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing,</p>

        <p><strong>Zhe Kong</strong>, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo,</p>

        <p><em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024.</em></p>

        <p><a href="https://ieeexplore.ieee.org/document/10654388"><strong>[paper]</strong></a> <a href="https://arxiv.org/abs/2401.01102"><strong>[arxiv]</strong></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TNNLS</div><img src="images/paper/dfdm.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>Taming Self-Supervised Learning for Presentation Attack Detection: De-Folding and De-Mixing,</p>

        <p><strong>Zhe Kong</strong>, Wentian Zhang, Feng Liu, Wenhan Luo, Haozhe Liu, Linlin Shen, Raghavendra Ramachandra,</p>

        <p><em>IEEE Transactions on Neural Networks and learning systems (TNNLS), 2023.</em></p>

        <p><a href="https://ieeexplore.ieee.org/abstract/document/10051654"><strong>[paper]</strong></a> <a href="https://arxiv.org/abs/2109.04100"><strong>[arxiv]</strong></a> <a href="https://github.com/kongzhecn/dfdm"><strong>[code]</strong></a>  <a href="https://github.com/kongzhecn/dfdm"><img src="https://img.shields.io/github/stars/kongzhecn/dfdm?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">TIFS</div><img src="images/paper/cfd-pad.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <ul>
      <li>
        <p>Fingerprint Presentation Attack Detection by Channel-Wise Feature Denoising,</p>

        <p>Feng Liu, <strong>Zhe Kong</strong>, Haozhe Liu, Wentian Zhang, Linlin Shen,</p>

        <p><em>IEEE Transactions on Information Forensics and Security (TIFS), 2022.</em></p>

        <p><a href="https://ieeexplore.ieee.org/abstract/document/9851680"><strong>[paper]</strong></a> <a href="https://arxiv.org/abs/2111.07620"><strong>[arxiv]</strong></a> <a href="https://github.com/kongzhecn/cfd-pad"><strong>[code]</strong></a>  <a href="https://github.com/kongzhecn/cfd-pad"><img src="https://img.shields.io/github/stars/kongzhecn/cfd-pad?style=social" alt="GitHub" /></a></p>
      </li>
    </ul>

  </div>
</div>

<h2 id="academic-services">Academic Services</h2>
<ul>
  <li><strong>Reviewer</strong>: TCSVT, TMM, PR, ICLR, NIPS, AAAI, ACM MM, ACL-ARR, ICASSP.</li>
</ul>

<h2 id="-internships">ğŸ’» Internships</h2>
<ul>
  <li><strong>2024.06 - now</strong>, Research Intern, Soul APP, Shanghai, China.</li>
  <li><strong>2024.02 - 2024.05</strong>, Research Intern, bilibili Inc, Shanghai, China.</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/qianqianai/qianqianai.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
