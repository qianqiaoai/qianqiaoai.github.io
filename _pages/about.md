---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>



My name is Qian Qiao (ä¹”è°¦). I am currently a researcher at Soul AILab. I received my Master's degree from Soochow University in July 2025, supervised by Professors [Fanzhang Li](https://scst.suda.edu.cn/0e/e0/c11250a528096/page.htm). My previous research focused on multimodal understanding, image generation, text spotting, and few-shot learning. Additionally, I have four years of experience in blockchain technology and investment. **Currently, my primary research interests are real-time video generation and dLLMs, and I am actively seeking collaborators in these fields!**


# ğŸ”¥ News
- *2025.01*: ğŸ‰ `QPruner:ProbabilisticDecision Quantization for StructuredPruning in Large Language Models` is accepted by NAACL 2025.
- *2024.12*: ğŸ‰ `AIM: Let Any Multimodal Large Language Models Embrace Efficient In-Context Learning` is accepted by AAAI 2024.
- *2024.11*: ğŸ‰ Invited by CogSci, ICME and IJCNN as Reviewer.
- *2024.10*: ğŸ‰ ğŸ”¥ğŸ”¥ğŸ”¥ I honoured the national scholarship.
- *2024.07*: ğŸ‰ Invited by AAAI as Reviewer.
- *2024.08*: ğŸ‰ One paper is accepted by PRICAI 2024.
- *2024.07*: ğŸ‰ ğŸ”¥ğŸ”¥ğŸ”¥ `DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training` is accepted by ACM MM 2024.
- *2024.07*: ğŸ‰ Invited by EMNLP as Reviewer.
- *2024.06*: ğŸ‰ Two papers are accepted by ICANN 2024.
- *2024.06*: ğŸ‰ Two papers are submitted to EMNLP 2024 as co-author.
- *2024.05*: ğŸ‰ Invited by NeurIPS as Reviewer.
- *2024.04*: ğŸ‰ Attending ICASSP2024 in Seoul, South Korea.
- *2024.04*: ğŸ‰ Two papers are accepted by ICME 2024.
- *2024.03*: ğŸ‰ Invited by MM as Program Reviewer.


# ğŸ“ Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><video src="https://github.com/user-attachments/assets/e55952e6-e1b2-44a5-9887-a89307a378da" width="320" controls loop></video></div></div>
<div class='paper-box-text' markdown="1">

- Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation,

  **Zhe Kong** *, Feng Gao *, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, Wenhan Luo

  *Conference on Neural Information Processing Systems (NeurIPS), 2025.*

  [**[arxiv]**](https://arxiv.org/abs/2505.22647) [**[code]**](https://github.com/MeiGen-AI/MultiTalk) [**[project]**](https://meigen-ai.github.io/multi-talk/)  [![GitHub](https://img.shields.io/github/stars/MeiGen-AI/MultiTalk?style=social)](https://github.com/MeiGen-AI/MultiTalk)


</div>
</div> -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EuroGraphics 2026</div><img src='images/papers/textflux.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis,

  Yu Xie, Jielei Zhang, **Qian Qiao**, Zhouhui Lian

  *EuroGraphics, 2026.*

  [**[arxiv]**](https://arxiv.org/abs/2505.17778) 
  [**[code]**](https://github.com/yyyyyxie/textflux) 
  [**[project]**](https://yyyyyxie.github.io/textflux-site/https://yyyyyxie.github.io/textflux-site/) 
  [![Arxiv]](https://img.shields.io/badge/arXiv-2505.17778-B31B1B?style=flat-square) 
  [![GitHub](https://img.shields.io/github/stars/yyyyyxie/textflux.svg?style=social&amp;label=Official)](https://github.com/yyyyyxie/textflux) 
  [![Citations](https://img.shields.io/badge/Citations-7-007EC6?style=flat-square)](https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=TextFlux%3A+An+OCR-Free+DiT+Model+for+High-Fidelity+Multilingual+Scene+Text+Synthesis&btnG=) 
  [![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Checkpoints-FFD21E?style=flat-square)](https://huggingface.co/yyyyyxie/textflux)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='images/paper/dnts.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training,

  **Qian Qiao**, Yu Xie, Jun Gao, Jiaqin Fan

  *ACM MM, 2024.*

  [**[arxiv]**](https://qianqiaoai.github.io/projects/dntextspotter/DNTextSpotter.pdf)  
  [**[code]**](https://github.com/yyyyyxie/DNTextSpotter)  
  [**[project]**](https://qianqiaoai.github.io/projects/dntextspotter)  
  [![GitHub](https://img.shields.io/github/stars/yyyyyxie/DNTextSpotter.svg?style=social&amp;label=Official)](https://github.com/yyyyyxie/DNTextSpotter)  
  [![Citations](https://img.shields.io/badge/Citations-21-007EC6?style=flat-square)](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=DNTextSpotter%3A+Arbitrary-Shaped+Scene+Text+Spotting+via+Improved+Denoising+Training&btnG=)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Technical Report</div><img src='images/paper/transdiff.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

- Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression

  Dingcheng Zhen', **Qian Qiao'**, Tan Yu, Xu Zhen, Kangxi Wu,

  *Technical Report, 2025.*

  [**[arxiv]**](https://arxiv.org/pdf/2506.09482) 
  [**[code]**](https://github.com/TransDiff/TransDiff) 
  <!-- [**[project]**](https://qqq.github.io/omg-project/)  -->
  [![GitHub](https://img.shields.io/github/stars/TransDiff/TransDiff.svg?style=social&amp;label=Official)](https://github.com/TransDiff/TransDiff)
  [![Citations](https://img.shields.io/badge/Citations-1-007EC6?style=flat-square)](https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Marrying+Autoregressive+Transformer+and+Diffusion+with+Multi-Reference+Autoregression&btnG=)
</div>
</div>


# ğŸ†š Academic Competitions
* **2024.06**: My collaborator Yu Xie and I won **three first places** and **one second place** in the **ICADR2024-Text Map** Challenge (ICADR is one of the most authoritative conferences in the field of OCR), and we have been **invited** to present a technical report at ICADR2024.

# ğŸ’» Internships
<!-- - <img src="images/tencent_ailab.jpg" alt="sym" width="4%"> *2024.04 - 2024.09*, [AI Lab CVC](https://github.com/AILab-CVC), Tencent, Research Intern, supervised by Dr. [Yong Zhang](https://yzhang2016.github.io/) -->
* **2024.06 - 2024.10w**, Research Intern, Soul APP, Shanghai, China.
* **2024.02 - 2024.05**, Research Intern, bilibili Inc, Shanghai, China.

# ğŸ– Honors and Awards
- 2025 Outstanding Graduate Award (Rate<5%)
- National Scholarship, 2024 (MS. student)
- Special Grade Scholarship, 2023 (MS. student)

# Visitors
<div id="clustrmaps-container" style="width: 500px; height: 500px;">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=4xiJoi-pCQJ8v08IPqo_ew00ryTF5iXNnlQqK-PIC_Y&cl=ffffff&w=a"></script>
</div>

